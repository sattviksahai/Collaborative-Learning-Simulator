{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Experiment ##\n",
    "### Dataset: Labeled Faces in the Wild ###\n",
    "### Experiment: Two party training for gender classification ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sattvik/envs/pytorch_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sattvik/envs/pytorch_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sattvik/envs/pytorch_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sattvik/envs/pytorch_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sattvik/envs/pytorch_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sattvik/envs/pytorch_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import cl_simulator.server as server\n",
    "import cl_simulator.workerclass as worker\n",
    "import cl_simulator.workerhandler as wh\n",
    "import cl_simulator.topology_utils as tu\n",
    "\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "# epochs = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "server_learning_rate = 0.05\n",
    "num_workers = 2\n",
    "local_iterations = 2\n",
    "\n",
    "data_path = '../data/lfw/data/'\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "!rm -rf ./runs/experiment_2\n",
    "writer = SummaryWriter('runs/experiment_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Data between workers ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sattvik/envs/pytorch_env/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/sattvik/envs/pytorch_env/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/sattvik/envs/pytorch_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3326: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries with worker 1: 5555, entries with worker 2: 5046, entries in training set: 2542, total entries: 13143\n"
     ]
    }
   ],
   "source": [
    "attributes_df = pd.read_csv(data_path+'lfw_attributes.txt')\n",
    "\n",
    "all_names = attributes_df.person.unique()\n",
    "tt_msk = np.random.rand(len(all_names)) < 0.8\n",
    "temp_train_names = all_names[tt_msk]\n",
    "test_names = all_names[~tt_msk]\n",
    "del all_names, tt_msk\n",
    "train_val_df = attributes_df.loc[attributes_df['person'].isin(temp_train_names)]\n",
    "test_df = attributes_df.loc[attributes_df['person'].isin(test_names)]\n",
    "\n",
    "# add column to indicate split\n",
    "train_val_df['target'] = 0\n",
    "# allocate half the people to the target\n",
    "names = train_val_df['person'].drop_duplicates()\n",
    "target_worker_names = names.sample(frac=1)[:int(len(names)/2)]\n",
    "target_worker_names = target_worker_names.reset_index(drop=True)\n",
    "\n",
    "# populate target field\n",
    "for index, row in train_val_df.iterrows():\n",
    "    if row['person'] in target_worker_names.values:\n",
    "        train_val_df['target'][index] = 1\n",
    "\n",
    "# print distribution of data\n",
    "print(\"entries with worker 1: {}, entries with worker 2: {}, entries in training set: {}, total entries: {}\".format(sum(train_val_df['target']==1), sum(train_val_df['target']==0), len(test_df), len(attributes_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset class ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFWDataset(Dataset):\n",
    "    \"\"\"LFW dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path, attributes_df, transform=None):\n",
    "        self.attributes_df = attributes_df\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.attributes_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.data_path, \"lfw_home/lfw_funneled\", self.attributes_df.iloc[idx]['person'].replace(' ', '_'),\"{}_{:04d}.jpg\".format(self.attributes_df.iloc[idx]['person'].replace(' ', '_'),self.attributes_df.iloc[idx]['imagenum']))\n",
    "#         img = torch.from_numpy(cv2.imread(img_path))\n",
    "        img = Image.open(img_path, mode='r')\n",
    "        \n",
    "        label = self.attributes_df.iloc[idx]['Male']>0\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
    "        self.pool1 = nn.MaxPool2d(4,4)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
    "        self.pool2 = nn.MaxPool2d(4,4)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "#         self.conv4 = nn.Conv2d(32, 32, 5)\n",
    "        self.fc1 = nn.Linear(800, 512)\n",
    "        self.fc2 = nn.Linear(512, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.dropout_layer1 = nn.Dropout(p=0.6)\n",
    "        self.dropout_layer2 = nn.Dropout(p=0.5)\n",
    "#         self.dropout_layer3 = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "#         x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.dropout_layer1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_layer2(x)\n",
    "#         x = self.dropout_layer3(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "def define_model():\n",
    "    return ResNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, and Evaluation functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_evaluation(val_model, dataloader):\n",
    "    with torch.no_grad():\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        for batch_idx, (data, target) in tqdm_notebook(enumerate(dataloader), total=len(dataloader)):\n",
    "            # move data batch to GPU\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            # forward pass\n",
    "            output = val_model(data)\n",
    "            loss = F.binary_cross_entropy(output, target.unsqueeze(1))\n",
    "            # compute average loss an accuracy\n",
    "            output = output.to('cpu')\n",
    "            target = target.to('cpu')\n",
    "            current_acc = torch.tensor(((output>0.5)== torch.tensor(target.unsqueeze(1), dtype=torch.bool)).sum(), dtype=torch.float)/torch.tensor(len(target), dtype=torch.float)\n",
    "            epoch_loss = ((epoch_loss*batch_idx) + loss.item())/(batch_idx+1)\n",
    "            epoch_accuracy = ((epoch_accuracy*batch_idx) + current_acc.item())/(batch_idx+1)\n",
    "    print(\"testing loss: {} and testing accuracy: {}\".format(epoch_loss, epoch_accuracy))\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_validation(val_model, dataloader):\n",
    "    with torch.no_grad():\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        for batch_idx, (data, target) in tqdm_notebook(enumerate(dataloader), total=len(dataloader)):\n",
    "            # move data batch to GPU\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            # forward pass\n",
    "            output = val_model(data)\n",
    "#             print(output, target.unsqueeze(1))\n",
    "            loss = F.binary_cross_entropy(output, target.unsqueeze(1))\n",
    "            # compute average loss an accuracy\n",
    "            output = output.to('cpu')\n",
    "            target = target.to('cpu')\n",
    "            current_acc = torch.tensor(((output>0.5)== torch.tensor(target.unsqueeze(1), dtype=torch.bool)).sum(), dtype=torch.float)/torch.tensor(len(target), dtype=torch.float)\n",
    "            epoch_loss = ((epoch_loss*batch_idx) + loss.item())/(batch_idx+1)\n",
    "            epoch_accuracy = ((epoch_accuracy*batch_idx) + current_acc.item())/(batch_idx+1)\n",
    "    print(\"val loss: {} and val accuracy: {}\".format(epoch_loss, epoch_accuracy))\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_training(train_model, dataloader, optimizer, local_iters):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    for batch_idx, (data, target) in tqdm_notebook(enumerate(dataloader), total=len(dataloader)):\n",
    "        # move data batch to GPU\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        output = train_model(data)\n",
    "        loss = F.binary_cross_entropy(output, target.unsqueeze(1))\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # compute average loss an accuracy\n",
    "        output = output.to('cpu')\n",
    "        target = target.to('cpu')\n",
    "        current_acc = torch.tensor(((output>0.5)== torch.tensor(target.unsqueeze(1), dtype=torch.bool)).sum(), dtype=torch.float)/torch.tensor(len(target), dtype=torch.float)\n",
    "        epoch_loss = ((epoch_loss*batch_idx) + loss.item())/(batch_idx+1)\n",
    "        epoch_accuracy = ((epoch_accuracy*batch_idx) + current_acc.item())/(batch_idx+1)\n",
    "        if batch_idx%local_iters:\n",
    "            yield epoch_loss, epoch_accuracy, False\n",
    "    print(\"train loss: {} and train accuracy: {}\".format(epoch_loss, epoch_accuracy))\n",
    "    return epoch_loss, epoch_accuracy, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare genuine worker ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class target_worker(worker.base_workerclass):\n",
    "    def __init__(self, name, attributes_df, model):\n",
    "        super().__init__(name, False)\n",
    "        self.worker_attributes_df = attributes_df[attributes_df['target']==1]\n",
    "        print(\"initializing genuine worker node with \",len(self.worker_attributes_df),\" data points\")\n",
    "        self.model = model\n",
    "        self.local_iters = local_iterations\n",
    "        # train val split\n",
    "        all_names = self.worker_attributes_df.person.unique()\n",
    "        tt_msk = np.random.rand(len(all_names)) < 0.8\n",
    "        train_names = all_names[tt_msk]\n",
    "        val_names = all_names[~tt_msk]\n",
    "        del all_names, tt_msk\n",
    "        \n",
    "        # set optimizer\n",
    "        self.set_optim()\n",
    "        # create train val and test dataframes\n",
    "        train_df = self.worker_attributes_df.loc[self.worker_attributes_df['person'].isin(train_names)]\n",
    "        val_df = self.worker_attributes_df.loc[self.worker_attributes_df['person'].isin(val_names)]\n",
    "        \n",
    "        train_dataset = LFWDataset(data_path, train_df, transform=transforms.Compose([\n",
    "#                                                     transforms.RandomResizedCrop(224),\n",
    "                                                    transforms.RandomHorizontalFlip(),\n",
    "                                                    transforms.ToTensor()\n",
    "                                                    ]))\n",
    "        val_dataset = LFWDataset(data_path, val_df, transform=transforms.Compose([\n",
    "#                                                             transforms.RandomResizedCrop(224),\n",
    "                                                            transforms.RandomHorizontalFlip(),\n",
    "                                                            transforms.ToTensor()]))\n",
    "        del train_df, val_df\n",
    "        \n",
    "        self.train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        self.val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        print(len(self.train_dataloader), len(self.val_dataloader))\n",
    "    \n",
    "    def set_param(self, w):\n",
    "        self.model.load_state_dict(w)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def set_optim(self):\n",
    "        self.optim = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def client_update(self, global_epoch):\n",
    "        global writer\n",
    "        self.model = self.model.cuda()\n",
    "        prev_w = copy.deepcopy(self.model.state_dict())\n",
    "        # unfreeze layers\n",
    "#         if 5 == global_epoch:\n",
    "#             self.model.unfreeze_layer3()\n",
    "#         if 20 == global_epoch:\n",
    "#             self.model.unfreeze_layer2()\n",
    "#         if 50 == global_epoch:\n",
    "#             self.model.unfreeze_layer1()\n",
    "#         for epoch in range(self.local_iters):\n",
    "            # run train and val epochs\n",
    "#             print(\"sub-epoch: {}\".format(epoch))\n",
    "        self.model.train()\n",
    "        train_loss, train_acc, run_val = perform_training(self.model, self.train_dataloader, self.optim, self.local_iters)\n",
    "        writer.add_scalar('training loss_'+self.name, train_loss, (global_epoch*self.local_iters)+epoch)\n",
    "        writer.add_scalar('training accuracy_'+self.name, train_acc, (global_epoch*self.local_iters)+epoch)\n",
    "        self.model.eval()\n",
    "        if run_val:\n",
    "            val_loss, val_acc = perform_validation(self.model, self.val_dataloader)\n",
    "            writer.add_scalar('validation loss_'+self.name, val_loss, (global_epoch*self.local_iters)+epoch)\n",
    "            writer.add_scalar('validation accuracy_'+self.name, val_acc, (global_epoch*self.local_iters)+epoch)\n",
    "        graddif = OrderedDict()\n",
    "        for (item1, item2) in zip(self.model.state_dict().items(),prev_w.items()):\n",
    "            key1=item1[0]\n",
    "            value1=item1[1]\n",
    "            key2=item2[0]\n",
    "            value2=item2[1]\n",
    "            diffval = value1-value2\n",
    "            graddif.update({key1:diffval.cpu()})\n",
    "        self.model = self.model.cpu()\n",
    "        return graddif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare malicious worker ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class malicious_worker(worker.base_workerclass):\n",
    "#     def __init__(self, attributes_df, model):\n",
    "#         super().__init__(True)\n",
    "#         self.worker_attributes_df = attributes_df[attributes_df['target']==0]\n",
    "#         print(\"initializing malicious worker node with \",len(self.worker_attributes_df),\" data points\")\n",
    "#         self.model = model\n",
    "#         self.local_iters = 5\n",
    "    \n",
    "#     def set_param(self, w):\n",
    "#         self.model.load_state_dict(w)\n",
    "    \n",
    "#     def set_optim(self):\n",
    "#         self.optim = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "    \n",
    "#     def client_update(self):\n",
    "#         print('ss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize components of our simulations ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing genuine worker node with  5555  data points\n",
      "70 18\n",
      "initializing genuine worker node with  5555  data points\n",
      "63 24\n"
     ]
    }
   ],
   "source": [
    "server1 = server.server(server_learning_rate)\n",
    "workers = wh.workerhandler([target_worker(\"w1\", train_val_df,define_model()),target_worker(\"w2\", train_val_df,define_model())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = tu.topology_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network topology ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.connect_star(server1, workers.get_all_workers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = tm.plot_topology()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "training on worker:  w1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:50",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-46a9db0dcca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mnew_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mnew_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserver1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Stevens/ShusenWang/Collaborative-Learning-Simulator/cl_simulator/workerhandler.py\u001b[0m in \u001b[0;36mperform_updates\u001b[0;34m(self, global_epoch)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mworker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training on worker: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mgradavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-3c4c193f6b47>\u001b[0m in \u001b[0;36mclient_update\u001b[0;34m(self, global_epoch)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclient_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mprev_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# unfreeze layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/pytorch_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/pytorch_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/pytorch_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/pytorch_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/pytorch_env/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    178\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0m_cudart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudaGetErrorName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:50"
     ]
    }
   ],
   "source": [
    "# initialize server weights as model average\n",
    "server1.set_init_weights(workers.get_average_weights())\n",
    "\n",
    "# start training\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    new_grad = workers.perform_updates(epoch)\n",
    "    new_w = server1.aggregate(new_grad)\n",
    "    workers.set_param(new_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate final model\n",
    "eval_model = define_model()\n",
    "eval_model.load_state_dict(new_w)\n",
    "eval_model.eval()\n",
    "eval_model = eval_model.cuda()\n",
    "torch.save(eval_model.state_dict(), \"models/experiment2_model.pt\")\n",
    "\n",
    "test_dataset = LFWDataset(data_path, test_df, transform=transforms.Compose([\n",
    "#                                                     transforms.RandomResizedCrop(224),\n",
    "                                                    transforms.RandomHorizontalFlip(),\n",
    "                                                    transforms.ToTensor()\n",
    "                                                    ]))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "test_loss, test_acc = perform_evaluation(eval_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
